{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Walk Through Ensemble Models\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. Please check the pdf file for more details.*\n",
    "\n",
    "In this exercise you will:\n",
    "\n",
    "- get to know a useful package **pandas** for data analysis/preprocessing\n",
    "- implement **decision tree** and apply it to a Titanic dataset\n",
    "- implement a whole bunch of **ensemble methods**, including **random forest, and adaboost**, and apply them to a Titanic dataset\n",
    "\n",
    "Please note that **YOU CANNOT USE ANY MACHINE LEARNING PACKAGE SUCH AS SKLEARN** for any homework, unless you are asked to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports\n",
    "from scipy import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first do some data preprocessing\n",
    "\n",
    "Here we use [pandas](https://pandas.pydata.org/) to do data preprocessing. Pandas is a very popular and handy package for data science or machine learning. You can also refer to this official guide for pandas: [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (1047, 11) test shape: (262, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hays, Miss. Margaret Bechstein</td>\n",
       "      <td>female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11767</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>C54</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Holm, Mr. John Fredrik Alexander</td>\n",
       "      <td>male</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C 7075</td>\n",
       "      <td>6.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Hansen, Mr. Claus Peter</td>\n",
       "      <td>male</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>350026</td>\n",
       "      <td>14.1083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Survived                              Name     Sex   Age  SibSp  \\\n",
       "0       1         1    Hays, Miss. Margaret Bechstein  female  24.0      0   \n",
       "1       3         0  Holm, Mr. John Fredrik Alexander    male  43.0      0   \n",
       "2       3         0           Hansen, Mr. Claus Peter    male  41.0      2   \n",
       "\n",
       "   Parch  Ticket     Fare Cabin Embarked  \n",
       "0      0   11767  83.1583   C54        C  \n",
       "1      0  C 7075   6.4500   NaN        S  \n",
       "2      0  350026  14.1083   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read titanic train and test data\n",
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "\n",
    "print(\"train shape: {} test shape: {}\".format(train.shape, test.shape))\n",
    "# Showing overview of the train dataset\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deal with missing values and transform to discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from: https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset\n",
    "full_data = [train, test]\n",
    "\n",
    "# Feature that tells whether a passenger had a cabin on the Titanic\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# Remove all NULLS in the Fare column\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "\n",
    "# Remove all NULLS in the Age column\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    # Next line has been improved to avoid warning\n",
    "    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_elements = ['Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Has_Cabin</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Survived  Sex  Age  Parch  Fare  Embarked  Has_Cabin  FamilySize  \\\n",
       "0       1         1    0    1      0     3         1          1           1   \n",
       "1       3         0    1    2      0     0         0          0           1   \n",
       "2       3         0    1    2      0     1         0          0           3   \n",
       "3       3         0    1    0      0     0         2          0           1   \n",
       "4       2         0    1    2      0     1         0          0           1   \n",
       "\n",
       "   IsAlone  Title  \n",
       "0        1      4  \n",
       "1        1      1  \n",
       "2        0      1  \n",
       "3        1      1  \n",
       "4        1      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the good thing of pd.DataFrame is that you can keep the column names along with the data, which can be beneficial for many case.\n",
    "\n",
    "Another good thing is that pd.DataFrame can be converted to np.array implicitely.\n",
    "\n",
    "Also, pd provides a lot of useful data manipulating methods for your convenience, though we may not use them in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (1047, 10), test: (262, 10)\n"
     ]
    }
   ],
   "source": [
    "X = train.drop(['Survived'], axis=1)\n",
    "y = train[\"Survived\"]\n",
    "X_test = test.drop(['Survived'], axis=1)\n",
    "y_test = test[\"Survived\"]\n",
    "print(\"train: {}, test: {}\".format(X.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_gt, y_pred):\n",
    "    return np.sum(y_gt == y_pred) / y_gt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived: 0.3878, Not Survivied: 0.6122\n"
     ]
    }
   ],
   "source": [
    "print(\"Survived: {:.4f}, Not Survivied: {:.4f}\".format(y.sum() / len(y), 1 - y.sum() / len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass  Sex  Age  Parch  Fare  Embarked  Has_Cabin  FamilySize  IsAlone  \\\n",
      "0       1    0    1      0     3         1          1           1        1   \n",
      "\n",
      "   Title  \n",
      "0      4  \n"
     ]
    }
   ],
   "source": [
    "print (X[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Now it's your turn to do some real coding. Please implement the decision tree model in **decision_tree.py**. The PDF file provides some hints for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Accuracy on train set: 0.8787010506208214\n",
      "Accuracy on test set: 0.7900763358778626\n"
     ]
    }
   ],
   "source": [
    "def calc_height(d):\n",
    "    now = 0\n",
    "    if type(d) == dict:\n",
    "        for t in list(d.values())[0].values():\n",
    "            now = max(now, calc_height(t))\n",
    "    return now + 1\n",
    "\n",
    "from decision_tree import DecisionTree\n",
    "\n",
    "# Plot the decision tree to get an intuition about how it makes decision\n",
    "#plt.figure(figsize=(10, 5))\n",
    "#dt.show()\n",
    "\n",
    "dt = DecisionTree(criterion='entropy', max_depth=10, min_samples_leaf=1, sample_feature=False)\n",
    "dt.fit(X, y)\n",
    "y_train_pred = dt.predict(X)\n",
    "print (calc_height(dt._tree))\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, dt.predict(X))))\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, dt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy 1 5 0.7992937733392289\n",
      "entropy 1 6 0.799393209786271\n",
      "entropy 1 7 0.7983865402979309\n",
      "entropy 1 8 0.8013493436089073\n",
      "entropy 1 9 0.7966667279789232\n",
      "entropy 1 10 0.7993397742309168\n",
      "entropy 2 5 0.8006302964652257\n",
      "entropy 2 6 0.8014027791642615\n",
      "entropy 2 7 0.8017326328019181\n",
      "entropy 2 8 0.7993429640235803\n",
      "entropy 2 9 0.7993365844382534\n",
      "entropy 2 10 0.7986762973569136\n",
      "entropy 3 5 0.7988859410708992\n",
      "entropy 3 6 0.8039949801715425\n",
      "entropy 3 7 0.7995988462654102\n",
      "entropy 3 8 0.7977485094838969\n",
      "entropy 3 9 0.7990754632319031\n",
      "entropy 3 10 0.8004119863579\n",
      "entropy 4 5 0.7996558803345193\n",
      "entropy 4 6 0.8042667448254687\n",
      "entropy 4 7 0.8024004590806377\n",
      "entropy 4 8 0.8045565018844514\n",
      "entropy 4 9 0.8052263583437815\n",
      "entropy 4 10 0.8018738862544673\n",
      "entropy 5 5 0.7980019528240249\n",
      "entropy 5 6 0.8051488739766676\n",
      "entropy 5 7 0.8045474267700199\n",
      "entropy 5 8 0.802619025815366\n",
      "entropy 5 9 0.8026158360227026\n",
      "entropy 5 10 0.8046190258153659\n",
      "entropy 6 5 0.7974416630045076\n",
      "entropy 6 6 0.8045042259411502\n",
      "entropy 6 7 0.8019027787345026\n",
      "entropy 6 8 0.7999743777798487\n",
      "entropy 6 9 0.8006410444465153\n",
      "entropy 6 10 0.7993013315278551\n",
      "entropy 7 5 0.7977438693166494\n",
      "entropy 7 6 0.8027347476654786\n",
      "entropy 7 7 0.8034014143321452\n",
      "entropy 7 8 0.8007315578728148\n",
      "entropy 7 9 0.8020712707914754\n",
      "entropy 7 10 0.8000680809988117\n",
      "entropy 8 5 0.7966474372929714\n",
      "entropy 8 6 0.8042460568913071\n",
      "entropy 8 7 0.8029063439726472\n",
      "entropy 8 8 0.8015602514686597\n",
      "entropy 8 9 0.8015666310539867\n",
      "entropy 8 10 0.8015666310539867\n",
      "entropy 9 5 0.7961458162632111\n",
      "entropy 9 6 0.8035947726430605\n",
      "entropy 9 7 0.8022646291023905\n",
      "entropy 9 8 0.8029249161837304\n",
      "entropy 9 9 0.8022582495170635\n",
      "entropy 9 10 0.8042646291023905\n",
      "infogain_ratio 1 5 0.800276759188008\n",
      "infogain_ratio 1 6 0.7992997804669218\n",
      "infogain_ratio 1 7 0.7959525949021028\n",
      "infogain_ratio 1 8 0.8083764484150884\n",
      "infogain_ratio 1 9 0.8006539879632333\n",
      "infogain_ratio 1 10 0.8013493436089073\n",
      "infogain_ratio 2 5 0.7989370462693477\n",
      "infogain_ratio 2 6 0.7999728267189153\n",
      "infogain_ratio 2 7 0.7966192615687694\n",
      "infogain_ratio 2 8 0.8043700688297614\n",
      "infogain_ratio 2 9 0.7993078954592463\n",
      "infogain_ratio 2 10 0.8026922463202311\n",
      "infogain_ratio 3 5 0.7996069027286778\n",
      "infogain_ratio 3 6 0.7999696369262519\n",
      "infogain_ratio 3 7 0.7957346079734011\n",
      "infogain_ratio 3 8 0.8051989335105413\n",
      "infogain_ratio 3 9 0.7991279616674788\n",
      "infogain_ratio 3 10 0.7977148247429254\n",
      "infogain_ratio 4 5 0.7996069027286777\n",
      "infogain_ratio 4 6 0.8006363035929185\n",
      "infogain_ratio 4 7 0.7947877536257419\n",
      "infogain_ratio 4 8 0.8049187458295488\n",
      "infogain_ratio 4 9 0.7980438933544921\n",
      "infogain_ratio 4 10 0.7960925707840384\n",
      "infogain_ratio 5 5 0.7989370462693477\n",
      "infogain_ratio 5 6 0.8013061600522488\n",
      "infogain_ratio 5 7 0.7939031000303737\n",
      "infogain_ratio 5 8 0.8056822199653745\n",
      "infogain_ratio 5 9 0.8002503530059315\n",
      "infogain_ratio 5 10 0.7978291684997926\n",
      "infogain_ratio 6 5 0.7996069027286777\n",
      "infogain_ratio 6 6 0.8006363035929185\n",
      "infogain_ratio 6 7 0.7978346907776911\n",
      "infogain_ratio 6 8 0.8054610432440092\n",
      "infogain_ratio 6 9 0.8011072737333176\n",
      "infogain_ratio 6 10 0.7951187344593587\n",
      "infogain_ratio 7 5 0.7996069027286778\n",
      "infogain_ratio 7 6 0.7999696369262519\n",
      "infogain_ratio 7 7 0.7950900454803513\n",
      "infogain_ratio 7 8 0.7991585270143726\n",
      "infogain_ratio 7 9 0.7992165304556356\n",
      "infogain_ratio 7 10 0.7937546495503147\n",
      "infogain_ratio 8 5 0.8002051601426621\n",
      "infogain_ratio 8 6 0.800567894340236\n",
      "infogain_ratio 8 7 0.795685113101672\n",
      "infogain_ratio 8 8 0.7985823737740054\n",
      "infogain_ratio 8 9 0.793225789916131\n",
      "infogain_ratio 8 10 0.7910415067196472\n",
      "infogain_ratio 9 5 0.8002051601426621\n",
      "infogain_ratio 9 6 0.8003530972041979\n",
      "infogain_ratio 9 7 0.7960032683771312\n",
      "infogain_ratio 9 8 0.7959343648608898\n",
      "infogain_ratio 9 9 0.7949055056564649\n",
      "infogain_ratio 9 10 0.7910852873545435\n",
      "gini 1 5 0.7959571646707644\n",
      "gini 1 6 0.795218252601504\n",
      "gini 1 7 0.7983864547554633\n",
      "gini 1 8 0.7919873021416027\n",
      "gini 1 9 0.7946635381862597\n",
      "gini 1 10 0.7993397742309168\n",
      "gini 2 5 0.796623831337431\n",
      "gini 2 6 0.7965547757275008\n",
      "gini 2 7 0.7963768853774729\n",
      "gini 2 8 0.7973365844382534\n",
      "gini 2 9 0.7953333946455899\n",
      "gini 2 10 0.7986667279789234\n",
      "gini 3 5 0.7925117402534488\n",
      "gini 3 6 0.7971689098234637\n",
      "gini 3 7 0.7972341292834828\n",
      "gini 3 8 0.7946980726217893\n",
      "gini 3 9 0.7953679290811195\n",
      "gini 3 10 0.7947012624144529\n",
      "gini 4 5 0.7958894207665759\n",
      "gini 4 6 0.7978988380508776\n",
      "gini 4 7 0.7992289815915474\n",
      "gini 4 8 0.8007088738931716\n",
      "gini 4 9 0.7987088738931715\n",
      "gini 4 10 0.8007088738931714\n",
      "gini 5 5 0.7961052254209784\n",
      "gini 5 6 0.7966284563607967\n",
      "gini 5 7 0.8006380257387871\n",
      "gini 5 8 0.8006380257387871\n",
      "gini 5 9 0.802644405324114\n",
      "gini 5 10 0.7999745488647839\n",
      "gini 6 5 0.795454026715199\n",
      "gini 6 6 0.7979868270330078\n",
      "gini 6 7 0.8006630630776648\n",
      "gini 6 8 0.8013297297443316\n",
      "gini 6 9 0.7986534936996744\n",
      "gini 6 10 0.7993233501590045\n",
      "gini 7 5 0.7938278320726869\n",
      "gini 7 6 0.7988186248790484\n",
      "gini 7 7 0.799491671131042\n",
      "gini 7 8 0.7981551480050453\n",
      "gini 7 9 0.7988186248790485\n",
      "gini 7 10 0.8001583377977086\n",
      "gini 8 5 0.7942112068081653\n",
      "gini 8 6 0.8004636483600465\n",
      "gini 8 7 0.7991303150267132\n",
      "gini 8 8 0.7991303150267131\n",
      "gini 8 9 0.8004636483600465\n",
      "gini 8 10 0.7991303150267132\n",
      "gini 9 5 0.7957876499515457\n",
      "gini 9 6 0.7990805635275814\n",
      "gini 9 7 0.7984138968609148\n",
      "gini 9 8 0.7990805635275814\n",
      "gini 9 9 0.7990805635275815\n",
      "gini 9 10 0.7984170866535782\n",
      "0.8083764484150884 infogain_ratio 1 8\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train the best DecisionTree(best val accuracy) that you can. You should choose some \n",
    "# hyper-parameters such as critertion, max_depth, and min_samples_in_leaf \n",
    "# according to the cross-validation result.\n",
    "# To reduce difficulty, you can use KFold here.\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "best_acc, best_min_samples_leaf, best_max_depth = 0, 0, 0\n",
    "best_method = \"\"\n",
    "for now_method in ['entropy', 'infogain_ratio', 'gini']:\n",
    "    for now_min_samples_leaf in range(1, 10):\n",
    "        for now_max_depth in range(5, 11):\n",
    "            dt = DecisionTree(criterion=now_method, max_depth=now_max_depth, min_samples_leaf=now_min_samples_leaf, sample_feature=False)\n",
    "            ave_acc = 0\n",
    "            for train_indice, valid_indice in kf.split(X, y):\n",
    "                X_train_fold, y_train_fold = X.loc[train_indice], y.loc[train_indice]\n",
    "                X_val_fold, y_val_fold = X.loc[valid_indice], y.loc[valid_indice]\n",
    "                dt.fit(X_train_fold, y_train_fold)\n",
    "                y_train_pred = dt.predict(X_train_fold) \n",
    "                y_valid_pred = dt.predict(X_val_fold)\n",
    "                ave_acc += accuracy(y_val_fold, y_valid_pred) * 0.7 + accuracy(y_train_fold, y_train_pred) * 0.3\n",
    "            ave_acc /= 5\n",
    "            print(now_method, now_min_samples_leaf, now_max_depth, ave_acc)\n",
    "            if ave_acc > best_acc:\n",
    "                best_acc = ave_acc\n",
    "                best_method = now_method\n",
    "                best_min_samples_leaf = now_min_samples_leaf\n",
    "                best_max_depth = now_max_depth\n",
    "    \n",
    "    \n",
    "# begin answer\n",
    "print (best_acc, best_method, best_min_samples_leaf, best_max_depth)\n",
    "# end answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.833810888252149\n",
      "Accuracy on test set: 0.8091603053435115\n"
     ]
    }
   ],
   "source": [
    "# report the accuracy on test set\n",
    "dt = DecisionTree(criterion=best_method, max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf, sample_feature=False)\n",
    "dt.fit(X, y)\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, dt.predict(X))))\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, dt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(criterion='infogain_ratio', max_depth=8, min_samples_leaf=1, sample_feature=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Please implement the random forest model in **random_forest.py**. The PDF file provides some hints for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.8404966571155683\n",
      "Accuracy on test set: 0.816793893129771\n"
     ]
    }
   ],
   "source": [
    "from random_forest import RandomForest\n",
    "\n",
    "base_learner = DecisionTree(criterion='entropy', max_depth=5, min_samples_leaf=5, sample_feature=True)\n",
    "rf = RandomForest(base_learner=base_learner, n_estimator=100, seed=2020)\n",
    "rf.fit(X, y)\n",
    "\n",
    "y_train_pred = rf.predict(X)\n",
    "\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, y_train_pred)))\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, rf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.8691499522445081\n",
      "Accuracy on test set: 0.8015267175572519\n"
     ]
    }
   ],
   "source": [
    "from random_forest import RandomForest\n",
    "\n",
    "base_learner = DecisionTree(criterion='entropy', max_depth=8, min_samples_leaf=5, sample_feature=True)\n",
    "rf = RandomForest(base_learner=base_learner, n_estimator=100, seed=2020)\n",
    "rf.fit(X, y)\n",
    "\n",
    "y_train_pred = rf.predict(X)\n",
    "\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, y_train_pred)))\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, rf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  infogain_ratio 1 3 78.1847% 79.2027% 77.7485%\n",
      "  infogain_ratio 1 4 79.9695% 80.9213% 79.5616%\n",
      "  infogain_ratio 1 5 80.8908% 82.2112% 80.3249%\n",
      "  infogain_ratio 1 6 80.9308% 83.2377% 79.9421%\n",
      "  infogain_ratio 1 7 80.1297% 84.3599% 78.3167%\n",
      "  infogain_ratio 1 8 80.3261% 86.1270% 77.8401%\n",
      "  infogain_ratio 1 9 80.4341% 87.8223% 77.2677%\n",
      "  infogain_ratio 2 3 78.1181% 79.2027% 77.6532%\n",
      "  infogain_ratio 2 4 79.9695% 80.9213% 79.5616%\n",
      "  infogain_ratio 2 5 80.8908% 82.2112% 80.3249%\n",
      "  infogain_ratio 2 6 80.9305% 83.2377% 79.9417%\n",
      "  infogain_ratio 2 7 79.8617% 84.3599% 77.9339%\n",
      "  infogain_ratio 2 8 80.3931% 86.1270% 77.9357%\n",
      "  infogain_ratio 2 9 80.8351% 87.8223% 77.8405%\n",
      "  infogain_ratio 3 3 78.2325% 78.9163% 77.9394%\n",
      "  infogain_ratio 3 4 79.6851% 81.0885% 79.0836%\n",
      "  infogain_ratio 3 5 81.0957% 82.4498% 80.5154%\n",
      "  infogain_ratio 3 6 80.1519% 83.0944% 78.8909%\n",
      "  infogain_ratio 3 7 81.1818% 84.0733% 79.9426%\n",
      "  infogain_ratio 3 8 80.4913% 86.0075% 78.1271%\n",
      "  infogain_ratio 3 9 80.0777% 87.0819% 77.0759%\n",
      "  infogain_ratio 4 3 78.6336% 79.5847% 78.2260%\n",
      "  infogain_ratio 4 4 80.0360% 80.9216% 79.6564%\n",
      "  infogain_ratio 4 5 80.5580% 81.9960% 79.9417%\n",
      "  infogain_ratio 4 6 80.4650% 83.0228% 79.3689%\n",
      "  infogain_ratio 4 7 80.4459% 84.0733% 78.8913%\n",
      "  infogain_ratio 4 8 80.9571% 85.5539% 78.9870%\n",
      "  infogain_ratio 4 9 80.3959% 86.5806% 77.7453%\n",
      "  infogain_ratio 5 3 78.8470% 79.6325% 78.5104%\n",
      "  infogain_ratio 5 4 80.5752% 81.1604% 80.3244%\n",
      "  infogain_ratio 5 5 80.7140% 82.0676% 80.1340%\n",
      "  infogain_ratio 5 6 81.0505% 83.1900% 80.1335%\n",
      "  infogain_ratio 5 7 80.7653% 84.0256% 79.3680%\n",
      "  infogain_ratio 5 8 81.0717% 85.2673% 79.2736%\n",
      "  infogain_ratio 5 9 80.4520% 86.1030% 78.0301%\n",
      "  infogain_ratio 6 3 78.8470% 79.6325% 78.5104%\n",
      "  infogain_ratio 6 4 79.7768% 80.2768% 79.5625%\n",
      "  infogain_ratio 6 5 80.6540% 82.0917% 80.0378%\n",
      "  infogain_ratio 6 6 80.4345% 83.1423% 79.2741%\n",
      "  infogain_ratio 6 7 80.9856% 83.6436% 79.8464%\n",
      "  infogain_ratio 6 8 80.6914% 84.8853% 78.8941%\n",
      "  infogain_ratio 6 9 80.0623% 85.9121% 77.5553%\n",
      "  infogain_ratio 7 3 78.7330% 79.9190% 78.2247%\n",
      "  infogain_ratio 7 4 79.2551% 80.3252% 78.7965%\n",
      "  infogain_ratio 7 5 80.4221% 82.2110% 79.6555%\n",
      "  infogain_ratio 7 6 80.0311% 82.6886% 78.8922%\n",
      "  infogain_ratio 7 7 80.8800% 83.7391% 79.6546%\n",
      "  infogain_ratio 7 8 80.8507% 84.5273% 79.2750%\n",
      "  infogain_ratio 7 9 80.7533% 85.1001% 78.8904%\n",
      "  infogain_ratio 8 3 78.5619% 79.7996% 78.0314%\n",
      "  infogain_ratio 8 4 80.4020% 81.4709% 79.9440%\n",
      "  infogain_ratio 8 5 81.1989% 81.9005% 80.8982%\n",
      "  infogain_ratio 8 6 81.4595% 83.2138% 80.7077%\n",
      "  infogain_ratio 8 7 80.1174% 83.6438% 78.6061%\n",
      "  infogain_ratio 8 8 80.6563% 84.5511% 78.9870%\n",
      "  infogain_ratio 8 9 79.6754% 85.2913% 77.2686%\n",
      "  infogain_ratio 9 3 78.0675% 78.8213% 77.7444%\n",
      "  infogain_ratio 9 4 80.3603% 81.1128% 80.0378%\n",
      "  infogain_ratio 9 5 80.0536% 82.0914% 79.1802%\n",
      "  infogain_ratio 9 6 80.9701% 83.1422% 80.0392%\n",
      "  infogain_ratio 9 7 80.7333% 83.6914% 79.4655%\n",
      "  infogain_ratio 9 8 80.5385% 84.5988% 78.7984%\n",
      "  infogain_ratio 9 9 80.4441% 84.9571% 78.5099%\n",
      "            gini 1 3 81.1761% 82.4977% 80.6097%\n",
      "            gini 1 4 81.7658% 83.5722% 80.9916%\n",
      "            gini 1 5 80.7188% 85.6495% 78.6056%\n",
      "            gini 1 6 81.1844% 87.2015% 78.6056%\n",
      "            gini 1 7 80.5858% 88.1088% 77.3616%\n",
      "            gini 1 8 80.8724% 88.3954% 77.6482%\n",
      "            gini 1 9 80.6699% 88.6103% 77.2668%\n",
      "            gini 2 3 81.1758% 82.4977% 80.6093%\n",
      "            gini 2 4 81.6991% 83.5722% 80.8963%\n",
      "            gini 2 5 80.5854% 85.6495% 78.4151%\n",
      "            gini 2 6 81.0498% 87.2015% 78.4133%\n",
      "            gini 2 7 80.6527% 88.1088% 77.4573%\n",
      "            gini 2 8 80.6051% 88.3954% 77.2663%\n",
      "            gini 2 9 80.2019% 88.6103% 76.5983%\n",
      "            gini 3 3 81.0120% 81.9485% 80.6106%\n",
      "            gini 3 4 81.0766% 83.5004% 80.0378%\n",
      "            gini 3 5 80.9494% 84.8615% 79.2727%\n",
      "            gini 3 6 81.3433% 86.1747% 79.2727%\n",
      "            gini 3 7 81.0684% 87.4881% 78.3172%\n",
      "            gini 3 8 80.5901% 87.6790% 77.5521%\n",
      "            gini 3 9 80.4550% 87.8940% 77.2668%\n",
      "            gini 4 3 80.8210% 81.7575% 80.4197%\n",
      "            gini 4 4 81.0548% 83.4288% 80.0374%\n",
      "            gini 4 5 80.8282% 85.1242% 78.9870%\n",
      "            gini 4 6 81.6309% 86.4613% 79.5607%\n",
      "            gini 4 7 80.9021% 87.1537% 78.2228%\n",
      "            gini 4 8 81.0089% 87.5119% 78.2219%\n",
      "            gini 4 9 80.9214% 87.4403% 78.1276%\n",
      "            gini 5 3 80.7805% 82.0678% 80.2288%\n",
      "            gini 5 4 81.6814% 83.2856% 80.9938%\n",
      "            gini 5 5 81.0401% 84.9331% 79.3716%\n",
      "            gini 5 6 81.1240% 85.8881% 79.0823%\n",
      "            gini 5 7 80.8392% 86.9386% 78.2251%\n",
      "            gini 5 8 80.8604% 87.0105% 78.2247%\n",
      "            gini 5 9 81.3180% 87.2014% 78.7965%\n",
      "            gini 6 3 79.9941% 81.9007% 79.1770%\n",
      "            gini 6 4 81.6811% 83.2856% 80.9934%\n",
      "            gini 6 5 81.3746% 84.7181% 79.9417%\n",
      "            gini 6 6 81.4537% 85.6495% 79.6555%\n",
      "            gini 6 7 81.4972% 86.4613% 79.3698%\n",
      "            gini 6 8 80.6760% 86.8434% 78.0328%\n",
      "            gini 6 9 80.8146% 87.0822% 78.1285%\n",
      "            gini 7 3 81.6416% 82.2587% 81.3771%\n",
      "            gini 7 4 80.7212% 82.7602% 79.8473%\n",
      "            gini 7 5 81.7850% 84.5272% 80.6097%\n",
      "            gini 7 6 81.2673% 85.6970% 79.3689%\n",
      "            gini 7 7 80.9128% 86.2941% 78.6065%\n",
      "            gini 7 8 81.1775% 86.5088% 78.8927%\n",
      "            gini 7 9 81.2605% 86.3419% 79.0827%\n",
      "            gini 8 3 80.9450% 81.9486% 80.5149%\n",
      "            gini 8 4 81.3178% 83.1900% 80.5154%\n",
      "            gini 8 5 81.6414% 84.2645% 80.5172%\n",
      "            gini 8 6 81.6469% 85.6256% 79.9417%\n",
      "            gini 8 7 80.9505% 86.1985% 78.7013%\n",
      "            gini 8 8 81.2247% 86.2226% 79.0827%\n",
      "            gini 8 9 80.8352% 86.4853% 78.4138%\n",
      "            gini 9 3 80.9404% 81.7099% 80.6106%\n",
      "            gini 9 4 81.2995% 82.9036% 80.6120%\n",
      "            gini 9 5 81.0209% 83.9781% 79.7535%\n",
      "            gini 9 6 81.6968% 85.1243% 80.2278%\n",
      "            gini 9 7 81.3886% 85.6495% 79.5625%\n",
      "            gini 9 8 80.7306% 85.9121% 78.5099%\n",
      "            gini 9 9 80.9296% 85.9121% 78.7943%\n",
      "         entropy 1 3 81.1837% 81.8531% 80.8968%\n",
      "         entropy 1 4 81.5277% 83.6677% 80.6106%\n",
      "         entropy 1 5 81.5011% 85.3630% 79.8460%\n",
      "         entropy 1 6 80.6868% 87.1058% 77.9357%\n",
      "         entropy 1 7 80.6004% 88.1565% 77.3620%\n",
      "         entropy 1 8 81.0799% 88.4193% 77.9344%\n",
      "         entropy 1 9 81.3914% 88.5626% 78.3181%\n",
      "         entropy 2 3 81.1840% 81.8531% 80.8972%\n",
      "         entropy 2 4 81.5950% 83.6677% 80.7068%\n",
      "         entropy 2 5 81.3008% 85.3630% 79.5598%\n",
      "         entropy 2 6 80.7538% 87.1058% 78.0314%\n",
      "         entropy 2 7 80.4667% 88.1565% 77.1711%\n",
      "         entropy 2 8 81.0138% 88.4193% 77.8401%\n",
      "         entropy 2 9 81.2578% 88.5626% 78.1271%\n",
      "         entropy 3 3 81.0465% 82.2826% 80.5167%\n",
      "         entropy 3 4 81.5904% 83.4289% 80.8025%\n",
      "         entropy 3 5 81.5699% 85.1480% 80.0365%\n",
      "         entropy 3 6 81.0545% 86.7716% 78.6042%\n",
      "         entropy 3 7 80.3865% 87.4404% 77.3634%\n",
      "         entropy 3 8 80.3440% 87.7507% 77.1697%\n",
      "         entropy 3 9 80.6098% 87.9657% 77.4573%\n",
      "         entropy 4 3 80.3178% 81.6378% 79.7521%\n",
      "         entropy 4 4 81.3963% 83.4528% 80.5149%\n",
      "         entropy 4 5 81.2258% 84.8854% 79.6573%\n",
      "         entropy 4 6 80.9714% 86.2704% 78.7004%\n",
      "         entropy 4 7 80.3288% 87.2492% 77.3630%\n",
      "         entropy 4 8 81.5778% 87.1776% 79.1779%\n",
      "         entropy 4 9 80.6964% 87.5835% 77.7448%\n",
      "         entropy 5 3 80.6921% 81.9964% 80.1331%\n",
      "         entropy 5 4 81.5299% 83.4527% 80.7059%\n",
      "         entropy 5 5 81.0918% 84.8854% 79.4659%\n",
      "         entropy 5 6 81.4085% 86.3898% 79.2736%\n",
      "         entropy 5 7 81.4658% 86.7955% 79.1816%\n",
      "         entropy 5 8 81.1978% 87.2493% 78.6042%\n",
      "         entropy 5 9 80.9554% 87.1060% 78.3194%\n",
      "         entropy 6 3 80.6952% 81.7813% 80.2297%\n",
      "         entropy 6 4 81.1767% 83.1662% 80.3240%\n",
      "         entropy 6 5 81.2020% 84.8137% 79.6541%\n",
      "         entropy 6 6 81.0642% 85.9122% 78.9866%\n",
      "         entropy 6 7 81.2557% 86.7716% 78.8918%\n",
      "         entropy 6 8 80.9703% 86.9389% 78.4124%\n",
      "         entropy 6 9 81.4002% 87.0343% 78.9856%\n",
      "         entropy 7 3 81.4471% 82.2826% 81.0891%\n",
      "         entropy 7 4 81.4916% 82.8794% 80.8968%\n",
      "         entropy 7 5 81.7545% 84.8617% 80.4229%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         entropy 7 6 81.8839% 85.5299% 80.3213%\n",
      "         entropy 7 7 81.6172% 86.4135% 79.5616%\n",
      "         entropy 7 8 81.5782% 86.5090% 79.4650%\n",
      "         entropy 7 9 80.7308% 86.5806% 78.2237%\n",
      "         entropy 8 3 80.5746% 81.8292% 80.0369%\n",
      "         entropy 8 4 82.2400% 83.1423% 81.8533%\n",
      "         entropy 8 5 81.1800% 84.2885% 79.8478%\n",
      "         entropy 8 6 81.1433% 85.5060% 79.2736%\n",
      "         entropy 8 7 81.0764% 86.1748% 78.8913%\n",
      "         entropy 8 8 81.3948% 86.3420% 79.2746%\n",
      "         entropy 8 9 81.2293% 86.4613% 78.9870%\n",
      "         entropy 9 3 81.0198% 81.9724% 80.6115%\n",
      "         entropy 9 4 81.7116% 83.1662% 81.0882%\n",
      "         entropy 9 5 81.2323% 84.2406% 79.9430%\n",
      "         entropy 9 6 82.0460% 85.1720% 80.7063%\n",
      "         entropy 9 7 81.1391% 85.9361% 79.0832%\n",
      "         entropy 9 8 81.7195% 85.8644% 79.9430%\n",
      "         entropy 9 9 81.3287% 86.1270% 79.2723%\n",
      "0.8223999233146568 entropy 8 4\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train the best RandomForest that you can. You should choose some \n",
    "# hyper-parameters such as max_depth, and min_samples_in_leaf \n",
    "# according to the cross-validation result.\n",
    "\n",
    "# For k = 10\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "best_acc, best_min_samples_leaf, best_max_depth = 0, 0, 0\n",
    "best_method = \"\"\n",
    "for now_method in ['infogain_ratio', 'gini', 'entropy']:\n",
    "    for now_min_samples_leaf in range(1, 10, 1):\n",
    "        for now_max_depth in range(3, 10, 1):\n",
    "            base_learner = DecisionTree(criterion=now_method, max_depth=now_max_depth, min_samples_leaf=now_min_samples_leaf, sample_feature=True)\n",
    "            rf = RandomForest(base_learner=base_learner, n_estimator=10, seed=2020)\n",
    "            ave_valid_acc, ave_train_acc, ave_com_acc = 0, 0, 0\n",
    "            for train_indice, valid_indice in kf.split(X, y):\n",
    "                X_train_fold, y_train_fold = X.loc[train_indice], y.loc[train_indice]\n",
    "                X_val_fold, y_val_fold = X.loc[valid_indice], y.loc[valid_indice]\n",
    "                rf.fit(X_train_fold, y_train_fold)\n",
    "                y_train_pred = rf.predict(X_train_fold) \n",
    "                y_valid_pred = rf.predict(X_val_fold)\n",
    "                train_acc = accuracy(y_train_fold, y_train_pred)\n",
    "                valid_acc = accuracy(y_val_fold, y_valid_pred)\n",
    "                ave_train_acc += train_acc\n",
    "                ave_valid_acc += valid_acc\n",
    "                ave_com_acc += valid_acc * 0.7 + train_acc * 0.3\n",
    "                \n",
    "            ave_train_acc /= 5\n",
    "            ave_valid_acc /= 5\n",
    "            ave_com_acc /= 5\n",
    "            print(\"%16s %d %d %.4f%% %.4f%% %.4f%%\" % (now_method, now_min_samples_leaf, now_max_depth, ave_com_acc*100, ave_train_acc*100, ave_valid_acc*100))\n",
    "            if ave_com_acc > best_acc:\n",
    "                best_acc = ave_com_acc\n",
    "                best_method = now_method\n",
    "                best_min_samples_leaf = now_min_samples_leaf\n",
    "                best_max_depth = now_max_depth\n",
    "    \n",
    "    \n",
    "# begin answer\n",
    "print (best_acc, best_method, best_min_samples_leaf, best_max_depth)\n",
    "# end answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.8299904489016237\n",
      "Accuracy on test set: 0.8206106870229007\n"
     ]
    }
   ],
   "source": [
    "# report the accuracy on test set\n",
    "# k=10\n",
    "# begin answer\n",
    "base_learner = DecisionTree(criterion=best_method, max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf, sample_feature=True)\n",
    "rf = RandomForest(base_learner=base_learner, n_estimator=10, seed=2020)\n",
    "# end answer\n",
    "rf.fit(X, y)\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, rf.predict(X))))\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, rf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  infogain_ratio 1 3 79.0561% 79.4415% 78.8909%\n",
      "  infogain_ratio 1 4 80.3463% 81.0650% 80.0383%\n",
      "  infogain_ratio 1 5 81.1680% 82.6885% 80.5163%\n",
      "  infogain_ratio 3 3 79.2239% 79.7756% 78.9875%\n",
      "  infogain_ratio 3 4 80.4344% 81.1365% 80.1335%\n",
      "  infogain_ratio 3 5 81.3780% 82.4975% 80.8982%\n",
      "  infogain_ratio 5 3 78.8731% 79.2740% 78.7013%\n",
      "  infogain_ratio 5 4 80.3886% 81.2083% 80.0374%\n",
      "  infogain_ratio 5 5 80.8788% 82.6168% 80.1340%\n",
      "  infogain_ratio 7 3 79.1757% 79.3937% 79.0823%\n",
      "  infogain_ratio 7 4 80.7734% 81.3755% 80.5154%\n",
      "  infogain_ratio 7 5 80.9483% 82.4020% 80.3254%\n",
      "  infogain_ratio 9 3 78.8872% 79.3221% 78.7008%\n",
      "  infogain_ratio 9 4 80.2936% 81.1126% 79.9426%\n",
      "  infogain_ratio 9 5 81.0966% 82.4497% 80.5167%\n",
      "            gini 1 3 81.6033% 82.1395% 81.3734%\n",
      "            gini 1 4 81.7639% 83.7870% 80.8968%\n",
      "            gini 1 5 81.8849% 85.5301% 80.3226%\n",
      "            gini 3 3 81.7229% 82.0918% 81.5648%\n",
      "            gini 3 4 82.0078% 83.9302% 81.1839%\n",
      "            gini 3 5 81.9693% 85.3629% 80.5149%\n",
      "            gini 5 3 81.6110% 82.1633% 81.3743%\n",
      "            gini 5 4 82.1390% 83.4766% 81.5657%\n",
      "            gini 5 5 82.0622% 85.0047% 80.8011%\n",
      "            gini 7 3 81.4696% 82.1394% 81.1825%\n",
      "            gini 7 4 82.1705% 83.3572% 81.6619%\n",
      "            gini 7 5 82.0432% 84.7182% 80.8968%\n",
      "            gini 9 3 81.4247% 82.2111% 81.0877%\n",
      "            gini 9 4 81.8359% 83.3571% 81.1839%\n",
      "            gini 9 5 82.0096% 84.3839% 80.9920%\n",
      "         entropy 1 3 81.5964% 82.1156% 81.3739%\n",
      "         entropy 1 4 81.8523% 83.8586% 80.9925%\n",
      "         entropy 1 5 81.9384% 85.4822% 80.4197%\n",
      "         entropy 3 3 81.5160% 82.0677% 81.2796%\n",
      "         entropy 3 4 81.8327% 83.5721% 81.0873%\n",
      "         entropy 3 5 81.9980% 85.4584% 80.5149%\n",
      "         entropy 5 3 81.6562% 82.0915% 81.4696%\n",
      "         entropy 5 4 82.1003% 83.5721% 81.4696%\n",
      "         entropy 5 5 81.8241% 85.1001% 80.4201%\n",
      "         entropy 7 3 81.3861% 82.3066% 80.9916%\n",
      "         entropy 7 4 82.1386% 83.4765% 81.5653%\n",
      "         entropy 7 5 81.8644% 84.7899% 80.6106%\n",
      "         entropy 9 3 81.3437% 82.1633% 80.9925%\n",
      "         entropy 9 4 82.0577% 83.4288% 81.4700%\n",
      "         entropy 9 5 81.8124% 84.1690% 80.8025%\n",
      "0.8217047728890462 gini 7 4\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train the best RandomForest that you can. You should choose some \n",
    "# hyper-parameters such as max_depth, and min_samples_in_leaf \n",
    "# according to the cross-validation result.\n",
    "\n",
    "# For k = 100\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "best_acc, best_min_samples_leaf, best_max_depth = 0, 0, 0\n",
    "best_method = \"\"\n",
    "for now_method in ['infogain_ratio', 'gini', 'entropy']:\n",
    "    for now_min_samples_leaf in range(1, 10, 2):\n",
    "        for now_max_depth in range(3, 6, 1):\n",
    "            base_learner = DecisionTree(criterion=now_method, max_depth=now_max_depth, min_samples_leaf=now_min_samples_leaf, sample_feature=True)\n",
    "            rf = RandomForest(base_learner=base_learner, n_estimator=100, seed=2020)\n",
    "            ave_valid_acc, ave_train_acc, ave_com_acc = 0, 0, 0\n",
    "            for train_indice, valid_indice in kf.split(X, y):\n",
    "                X_train_fold, y_train_fold = X.loc[train_indice], y.loc[train_indice]\n",
    "                X_val_fold, y_val_fold = X.loc[valid_indice], y.loc[valid_indice]\n",
    "                rf.fit(X_train_fold, y_train_fold)\n",
    "                y_train_pred = rf.predict(X_train_fold) \n",
    "                y_valid_pred = rf.predict(X_val_fold)\n",
    "                train_acc = accuracy(y_train_fold, y_train_pred)\n",
    "                valid_acc = accuracy(y_val_fold, y_valid_pred)\n",
    "                ave_train_acc += train_acc\n",
    "                ave_valid_acc += valid_acc\n",
    "                ave_com_acc += valid_acc * 0.7 + train_acc * 0.3\n",
    "                \n",
    "            ave_train_acc /= 5\n",
    "            ave_valid_acc /= 5\n",
    "            ave_com_acc /= 5\n",
    "            print(\"%16s %d %d %.4f%% %.4f%% %.4f%%\" % (now_method, now_min_samples_leaf, now_max_depth, ave_com_acc*100, ave_train_acc*100, ave_valid_acc*100))\n",
    "            if ave_com_acc > best_acc:\n",
    "                best_acc = ave_com_acc\n",
    "                best_method = now_method\n",
    "                best_min_samples_leaf = now_min_samples_leaf\n",
    "                best_max_depth = now_max_depth\n",
    "    \n",
    "    \n",
    "# begin answer\n",
    "print (best_acc, best_method, best_min_samples_leaf, best_max_depth)\n",
    "# end answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.8347659980897804\n",
      "Accuracy on test set: 0.8206106870229007\n"
     ]
    }
   ],
   "source": [
    "# report the accuracy on test set\n",
    "# k=100\n",
    "# begin answer\n",
    "base_learner = DecisionTree(criterion='gini', max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf, sample_feature=True)\n",
    "rf = RandomForest(base_learner=base_learner, n_estimator=100, seed=2020)\n",
    "# end answer\n",
    "rf.fit(X, y)\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, rf.predict(X))))\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, rf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "Please implement the adaboost model in **adaboost.py**. The PDF file provides some hints for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.8089780324737345\n",
      "Accuracy on test set: 0.8129770992366412\n",
      "Time: 41.196943283081055\n"
     ]
    }
   ],
   "source": [
    "from decision_tree import DecisionTree\n",
    "from adaboost import Adaboost\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "base_learner = DecisionTree(criterion='entropy', max_depth=1, min_samples_leaf=1, sample_feature=False)\n",
    "ada = Adaboost(base_learner=base_learner, n_estimator=100, seed=2020)\n",
    "ada.fit(X, y)\n",
    "\n",
    "y_train_pred = ada.predict(X)\n",
    "\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, y_train_pred)))\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, ada.predict(X_test))))\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print (\"Time:\", t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  infogain_ratio 1 1 80.9768% 81.1604% 80.8982%\n",
      "  infogain_ratio 1 2 80.5538% 84.4318% 78.8918%\n",
      "  infogain_ratio 1 3 80.6880% 87.5596% 77.7430%\n",
      "  infogain_ratio 3 1 80.9768% 81.1604% 80.8982%\n",
      "  infogain_ratio 3 2 80.4868% 84.4318% 78.7961%\n",
      "  infogain_ratio 3 3 80.3455% 87.5357% 77.2641%\n",
      "  infogain_ratio 5 1 80.9768% 81.1604% 80.8982%\n",
      "  infogain_ratio 5 2 80.1599% 84.2407% 78.4110%\n",
      "  infogain_ratio 5 3 80.4390% 87.8461% 77.2645%\n",
      "            gini 1 1 80.6669% 81.0173% 80.5167%\n",
      "            gini 1 2 81.0644% 85.2438% 79.2732%\n",
      "            gini 1 3 79.6206% 88.0134% 76.0237%\n",
      "            gini 3 1 80.6669% 81.0173% 80.5167%\n",
      "            gini 3 2 81.0640% 85.2438% 79.2727%\n",
      "            gini 3 3 79.6530% 87.8939% 76.1212%\n",
      "            gini 5 1 80.6669% 81.0173% 80.5167%\n",
      "            gini 5 2 81.1712% 84.9331% 79.5589%\n",
      "            gini 5 3 80.3509% 87.9894% 77.0772%\n",
      "         entropy 1 1 80.5501% 80.8501% 80.4215%\n",
      "         entropy 1 2 81.4017% 85.0284% 79.8473%\n",
      "         entropy 1 3 79.8755% 87.9655% 76.4083%\n",
      "         entropy 3 1 80.5501% 80.8501% 80.4215%\n",
      "         entropy 3 2 81.3350% 85.0284% 79.7521%\n",
      "         entropy 3 3 79.7867% 87.8939% 76.3121%\n",
      "         entropy 5 1 80.5501% 80.8501% 80.4215%\n",
      "         entropy 5 2 81.2322% 84.9091% 79.6564%\n",
      "         entropy 5 3 79.8755% 87.9655% 76.4083%\n",
      "0.8140166625266568 entropy 1 2\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train the best Adaboost that you can. You should choose some \n",
    "# hyper-parameters such as max_depth, and min_samples_in_leaf \n",
    "# according to the cross-validation result.\n",
    "# begin answer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "best_acc, best_min_samples_leaf, best_max_depth = 0, 0, 0\n",
    "best_method = \"\"\n",
    "for now_method in ['infogain_ratio', 'gini', 'entropy']:\n",
    "    for now_min_samples_leaf in range(1, 6, 2):\n",
    "        for now_max_depth in range(1, 4, 1):\n",
    "            base_learner = DecisionTree(criterion=now_method, max_depth=now_max_depth, min_samples_leaf=now_min_samples_leaf, sample_feature=False)\n",
    "            ada = Adaboost(base_learner=base_learner, n_estimator=50, seed=2020)\n",
    "            ave_valid_acc, ave_train_acc, ave_com_acc = 0, 0, 0\n",
    "            for train_indice, valid_indice in kf.split(X, y):\n",
    "                X_train_fold, y_train_fold = X.loc[train_indice], y.loc[train_indice]\n",
    "                X_val_fold, y_val_fold = X.loc[valid_indice], y.loc[valid_indice]\n",
    "                ada.fit(X_train_fold, y_train_fold)\n",
    "                y_train_pred = ada.predict(X_train_fold) \n",
    "                y_valid_pred = ada.predict(X_val_fold)\n",
    "                train_acc = accuracy(y_train_fold, y_train_pred)\n",
    "                valid_acc = accuracy(y_val_fold, y_valid_pred)\n",
    "                ave_train_acc += train_acc\n",
    "                ave_valid_acc += valid_acc\n",
    "                ave_com_acc += valid_acc * 0.7 + train_acc * 0.3\n",
    "                \n",
    "            ave_train_acc /= 5\n",
    "            ave_valid_acc /= 5\n",
    "            ave_com_acc /= 5\n",
    "            print(\"%16s %d %d %.4f%% %.4f%% %.4f%%\" % (now_method, now_min_samples_leaf, now_max_depth, ave_com_acc*100, ave_train_acc*100, ave_valid_acc*100))\n",
    "            if ave_com_acc > best_acc:\n",
    "                best_acc = ave_com_acc\n",
    "                best_method = now_method\n",
    "                best_min_samples_leaf = now_min_samples_leaf\n",
    "                best_max_depth = now_max_depth\n",
    "    \n",
    "    \n",
    "# begin answer\n",
    "print (best_acc, best_method, best_min_samples_leaf, best_max_depth)\n",
    "# end answer\n",
    "# end answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.8118433619866284\n",
      "Accuracy on test set: 0.8091603053435115\n"
     ]
    }
   ],
   "source": [
    "# report the accuracy on test set\n",
    "# begin answer\n",
    "# end answer\n",
    "base_learner = DecisionTree(criterion='infogain_ratio', max_depth=1, min_samples_leaf=3, sample_feature=False)\n",
    "ada = Adaboost(base_learner=base_learner, n_estimator=50, seed=2020)\n",
    "\n",
    "ada.fit(X, y)\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, ada.predict(X))))\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, ada.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "train_x = np.array(X)\n",
    "train_y = np.array(y)\n",
    "test_x = np.array(X_test)\n",
    "test_y = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes train accuracy: 0.766953199617956\n",
      "naive bayes test accuracy: 0.7557251908396947\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../../assignment1/bayes_decision_rule\")\n",
    "from posterior import posterior\n",
    "# Some modification must be done if use my own naive bayes, so I use sklearn.\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "print(\"naive bayes train accuracy:\", accuracy(clf.predict(train_x), train_y))\n",
    "print(\"naive bayes test accuracy:\", accuracy(clf.predict(test_x), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
      "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
      "           penalty=None, random_state=0, shuffle=False, tol=0.001,\n",
      "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "perceptron train accuracy: 0.6227316141356256\n",
      "perceptron test accuracy: 0.6183206106870229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf = Perceptron(shuffle=False)\n",
    "print (clf.fit(train_x, train_y))\n",
    "\n",
    "print(\"perceptron train accuracy:\", accuracy(clf.predict(train_x), train_y))\n",
    "print(\"perceptron test accuracy:\", accuracy(clf.predict(test_x), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "perceptron train accuracy: 0.7975167144221585\n",
      "perceptron test accuracy: 0.7862595419847328\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "print (clf.fit(train_x, train_y))\n",
    "\n",
    "print(\"perceptron train accuracy:\", accuracy(clf.predict(train_x), train_y))\n",
    "print(\"perceptron test accuracy:\", accuracy(clf.predict(test_x), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "perceptron train accuracy: 0.8185291308500477\n",
      "perceptron test accuracy: 0.8129770992366412\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "print (clf.fit(train_x, train_y))\n",
    "\n",
    "print(\"perceptron train accuracy:\", accuracy(clf.predict(train_x), train_y))\n",
    "print(\"perceptron test accuracy:\", accuracy(clf.predict(test_x), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n",
      "perceptron train accuracy: 0.8175740210124164\n",
      "perceptron test accuracy: 0.8282442748091603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier()\n",
    "print (clf.fit(train_x, train_y))\n",
    "\n",
    "print(\"perceptron train accuracy:\", accuracy(clf.predict(train_x), train_y))\n",
    "print(\"perceptron test accuracy:\", accuracy(clf.predict(test_x), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
      "                     weights='uniform')\n",
      "perceptron train accuracy: 0.828080229226361\n",
      "perceptron test accuracy: 0.7786259541984732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=2)\n",
    "print (clf.fit(train_x, train_y))\n",
    "\n",
    "print(\"perceptron train accuracy:\", accuracy(clf.predict(train_x), train_y))\n",
    "print(\"perceptron test accuracy:\", accuracy(clf.predict(test_x), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "perceptron train accuracy: 0.8767908309455588\n",
      "perceptron test accuracy: 0.8015267175572519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "print (clf.fit(train_x, train_y))\n",
    "\n",
    "print(\"perceptron train accuracy:\", accuracy(clf.predict(train_x), train_y))\n",
    "print(\"perceptron test accuracy:\", accuracy(clf.predict(test_x), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "perceptron train accuracy: 0.8767908309455588\n",
      "perceptron test accuracy: 0.8129770992366412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "print (clf.fit(train_x, train_y))\n",
    "\n",
    "print(\"perceptron train accuracy:\", accuracy(clf.predict(train_x), train_y))\n",
    "print(\"perceptron test accuracy:\", accuracy(clf.predict(test_x), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "perceptron train accuracy: 0.8099331423113658\n",
      "perceptron test accuracy: 0.8129770992366412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "print (clf.fit(train_x, train_y))\n",
    "\n",
    "print(\"perceptron train accuracy:\", accuracy(clf.predict(train_x), train_y))\n",
    "print(\"perceptron test accuracy:\", accuracy(clf.predict(test_x), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8213944603629417\n",
      "0.8091603053435115\n"
     ]
    }
   ],
   "source": [
    "from GBDT import GBDT\n",
    "from binary_decision_tree import BinaryDecisionTree\n",
    "\n",
    "base_learner = BinaryDecisionTree(max_depth = 5, min_samples_leaf = 3)\n",
    "gbdt = GBDT(base_learner = base_learner, n_estimator = 100, learning_rate = 0.6)\n",
    "\n",
    "gbdt.fit(X, y)\n",
    "print (accuracy(gbdt.predict(X),y))\n",
    "print (accuracy(gbdt.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  2.9122109413146973\n",
      "Average accuracy on train set: 0.8187201528175742\n",
      "Average accuracy on test set: 0.8076335877862595\n",
      "Average nodes: 44.4\n"
     ]
    }
   ],
   "source": [
    "# prune\n",
    "\n",
    "from decision_tree import DecisionTree\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "X_train, X_valid = X[:670], X[670:]\n",
    "y_train, y_valid = y[:670], y[670:]\n",
    "\n",
    "t1 = time.time()\n",
    "ave_train, ave_test, ave_node = 0, 0, 0\n",
    "\n",
    "for repeat in range(10):\n",
    "    dt = DecisionTree(criterion='entropy', max_depth=10, min_samples_leaf=repeat, sample_feature=False, last_cut = True)\n",
    "    #dt.fit(X, y)\n",
    "    dt.fit_with_cut(X_train, y_train, X_valid, y_valid)\n",
    "    ave_train += accuracy(y, dt.predict(X))\n",
    "    ave_test += accuracy(y_test, dt.predict(X_test))\n",
    "    ave_node += dt.calc_nodes(dt._tree)\n",
    "    \n",
    "ave_train /= 10\n",
    "ave_test /= 10\n",
    "ave_node /= 10\n",
    "\n",
    "t2 = time.time()\n",
    "print (\"time: \", t2-t1)\n",
    "print(\"Average accuracy on train set: {}\".format(ave_train))\n",
    "print(\"Average accuracy on test set: {}\".format(ave_test))\n",
    "print(\"Average nodes: {}\".format(ave_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8213944603629417\n",
      "0.8091603053435115\n"
     ]
    }
   ],
   "source": [
    "# continous\n",
    "\n",
    "from GBDT import GBDT\n",
    "from binary_decision_tree import BinaryDecisionTree\n",
    "\n",
    "base_learner = BinaryDecisionTree(max_depth = 5, min_samples_leaf = 3)\n",
    "gbdt = GBDT(base_learner = base_learner, n_estimator = 100, learning_rate = 0.6)\n",
    "\n",
    "gbdt.fit(X, y)\n",
    "pred_test = gbdt.predict(X_test)\n",
    "print (accuracy(gbdt.predict(X),y))\n",
    "print (accuracy(pred_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 0.8091603053435115\n",
      "200 0.865\n",
      "72 0.9027777777777778\n"
     ]
    }
   ],
   "source": [
    "#pred_test, real_test = gbdt.predict(X_test)\n",
    "\n",
    "def choose(th):\n",
    "    chosen_data = np.logical_or(real_test <= th, real_test >= 1 - th)\n",
    "    print (np.sum(chosen_data), accuracy(pred_test[chosen_data], y_test[chosen_data]))\n",
    "\n",
    "choose(0.5)\n",
    "choose(0.3)\n",
    "choose(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593353846.7905393\n",
      "1593353846.949146\n",
      "True\n",
      "Start\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-261-c4b38c6d7b57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\\\\\project\\code\\random_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[0midy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallel_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_origin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_parallel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python37\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python37\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python37\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python37\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python37\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from random_forest import RandomForest\n",
    "\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "pickle.dump(X, file = open(\"X\", \"wb\"))\n",
    "pickle.dump(y, file = open(\"y\", \"wb\"))\n",
    "\n",
    "print (time.time())\n",
    "base_learner = DecisionTree(criterion='entropy', max_depth=5, min_samples_leaf=5, sample_feature=True)\n",
    "rf = RandomForest(base_learner=base_learner, n_estimator=10, seed=2020, is_parallel = True)\n",
    "\n",
    "rf.fit(X[0:300],y[0:300])\n",
    "print (time.time())\n",
    "y_train_pred = rf.predict(X[0:300])\n",
    "print (time.time())\n",
    "\n",
    "#print(\"Accuracy on train set: {}\".format(accuracy(y, y_train_pred)))\n",
    "#print(\"Accuracy on test set: {}\".format(accuracy(y_test, rf.predict(X_test))))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
